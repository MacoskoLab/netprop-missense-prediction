{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f094bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import sparse\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903608f-80b8-4f2e-b9e6-53efca92d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "adata = sc.read_h5ad(\"../../resources/GSE158067/adata_GSE158067.h5ad\")\n",
    "target_tf_name = \"DNMT3A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract wildtype expression matrix and gene names\n",
    "adata_wildtype = adata[adata.obs[\"genotype\"] == \"Wild-type\"].copy()\n",
    "adata_wildtype.var.loc[target_tf_name, \"highly_variable\"] = True\n",
    "data_wildtype = adata_wildtype[:, adata_wildtype.var[\"highly_variable\"]].copy()\n",
    "\n",
    "# Use log-normalized data for GRN inference\n",
    "X = (\n",
    "    adata_wildtype.layers[\"log_normalized\"].toarray()\n",
    "    if hasattr(adata_wildtype.layers[\"log_normalized\"], \"toarray\")\n",
    "    else adata_wildtype.layers[\"log_normalized\"]\n",
    ")\n",
    "genes = list(adata_wildtype.var_names)\n",
    "\n",
    "target_tf_index = genes.index(target_tf_name)\n",
    "\n",
    "print(f\"Wildtype cells: {adata_wildtype.shape[0]}, Genes: {len(genes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb420748",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_wildtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5d24c-88fc-4510-99e4-5671dca52b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Gene Regulatory Network (GRN) Inference\n",
    "def train_single_rf_model(target_gene_idx, expression_matrix, num_trees, random_seed):\n",
    "    \"\"\"\n",
    "    Train a single Random Forest model for one target gene.\n",
    "    This function is designed to be called in parallel.\n",
    "    \"\"\"\n",
    "    target_expression = expression_matrix[:, target_gene_idx]\n",
    "\n",
    "    # Train Random Forest regressor\n",
    "    random_forest = RandomForestRegressor(\n",
    "        n_estimators=num_trees,\n",
    "        n_jobs=1,  # Use 1 job per process to avoid nested parallelization\n",
    "        random_state=random_seed + target_gene_idx,  # Ensure different seeds\n",
    "    )\n",
    "    random_forest.fit(expression_matrix, target_expression)\n",
    "\n",
    "    return target_gene_idx, random_forest.feature_importances_\n",
    "\n",
    "\n",
    "def infer_GRN_parallel(\n",
    "    expression_matrix, num_trees=50, random_seed=0, n_processes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Parallel version of GRN inference using multiprocessing with progress monitoring.\n",
    "\n",
    "    Parameters:\n",
    "    - expression_matrix: Gene expression matrix (cells x genes)\n",
    "    - num_trees: Number of trees in each Random Forest\n",
    "    - random_seed: Random seed for reproducibility\n",
    "    - n_processes: Number of processes to use (None = auto-detect)\n",
    "    \"\"\"\n",
    "    num_cells, num_genes = expression_matrix.shape\n",
    "    adjacency_matrix = np.zeros((num_genes, num_genes))\n",
    "\n",
    "    if n_processes is None:\n",
    "        n_processes = min(mp.cpu_count(), num_genes)\n",
    "\n",
    "    print(f\"Using {n_processes} processes for parallel GRN inference...\")\n",
    "    print(f\"Training Random Forest models for {num_genes} genes...\")\n",
    "\n",
    "    # Create partial function with fixed parameters\n",
    "    train_func = partial(\n",
    "        train_single_rf_model,\n",
    "        expression_matrix=expression_matrix,\n",
    "        num_trees=num_trees,\n",
    "        random_seed=random_seed,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use multiprocessing with progress monitoring\n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        # Use imap instead of map to get results as they complete\n",
    "        results_iter = pool.imap(train_func, range(num_genes))\n",
    "\n",
    "        # Collect results with progress bar\n",
    "        results = []\n",
    "        for result in tqdm(results_iter, total=num_genes, desc=\"Training RF models\"):\n",
    "            results.append(result)\n",
    "\n",
    "    # Collect results into adjacency matrix\n",
    "    for target_gene_idx, feature_importances in results:\n",
    "        adjacency_matrix[:, target_gene_idx] = feature_importances\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Parallel GRN inference completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "grn_file_h5ad = f\"../../results/grn_output/grn_adjacency_matrix_{target_tf_name}.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc4ec4-21d6-4df4-af30-6b9007a02fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parallel GRN Inference\n",
    "# print(\"Inferring gene regulatory network using parallel processing...\")\n",
    "\n",
    "# # Run parallel GRN inference\n",
    "# gene_regulatory_network = infer_GRN_parallel(\n",
    "#     X,\n",
    "#     num_trees=50,\n",
    "#     random_seed=0,\n",
    "#     n_processes=None,  # Auto-detect optimal number of processes\n",
    "# )\n",
    "\n",
    "# print(f\"GRN inferred with shape: {gene_regulatory_network.shape}\")\n",
    "\n",
    "# # Display basic network statistics\n",
    "# non_zero_edges = np.count_nonzero(gene_regulatory_network)\n",
    "# total_possible_edges = gene_regulatory_network.size\n",
    "# sparsity = (non_zero_edges / total_possible_edges) * 100\n",
    "\n",
    "# print(f\"Network statistics:\")\n",
    "# print(f\"  - Non-zero edges: {non_zero_edges:,}\")\n",
    "# print(f\"  - Network sparsity: {sparsity:.2f}%\")\n",
    "# print(f\"  - Max edge weight: {gene_regulatory_network.max():.6f}\")\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# output_dir = \"../../results/grn_output\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Convert to sparse matrix for efficient storage\n",
    "# grn_sparse = sparse.csr_matrix(gene_regulatory_network)\n",
    "\n",
    "# # Create AnnData object where:\n",
    "# # - X contains the GRN adjacency matrix\n",
    "# # - obs (observations) represent source genes\n",
    "# # - var (variables) represent target genes\n",
    "# grn_adata = sc.AnnData(\n",
    "#     X=grn_sparse, obs=pd.DataFrame(index=genes), var=pd.DataFrame(index=genes)\n",
    "# )\n",
    "\n",
    "# # Add metadata (convert to h5ad-compatible types)\n",
    "# grn_adata.uns[\"description\"] = (\n",
    "#     f\"Gene regulatory network adjacency matrix for {target_tf_name} perturbation\"\n",
    "# )\n",
    "# grn_adata.uns[\"target_tf\"] = target_tf_name\n",
    "# grn_adata.uns[\"matrix_shape\"] = list(\n",
    "#     gene_regulatory_network.shape\n",
    "# )  # Convert tuple to list\n",
    "# grn_adata.uns[\"inference_method\"] = \"Random Forest\"\n",
    "# grn_adata.uns[\"n_genes\"] = int(len(genes))  # Ensure it's a standard int\n",
    "\n",
    "# # Save as h5ad file\n",
    "# grn_adata.write(grn_file_h5ad)\n",
    "# print(f\"GRN adjacency matrix saved as AnnData h5ad file: {grn_file_h5ad}\")\n",
    "\n",
    "# print(f\"\\nMatrix shape: {gene_regulatory_network.shape}\")\n",
    "# print(f\"Number of genes: {len(genes)}\")\n",
    "# print(f\"Sparsity: {(grn_sparse.nnz / grn_sparse.size) * 100:.2f}% non-zero values\")\n",
    "# print(f\"File size reduction: sparse matrix is much more efficient for storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify the saved GRN matrix\n",
    "loaded_grn_adata = sc.read_h5ad(grn_file_h5ad)\n",
    "gene_regulatory_network = loaded_grn_adata.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def _best_gn_partition(G, target_num_communities=None, max_splits=50):\n",
    "    \"\"\"\n",
    "    Run Girvanâ€“Newman and return the partition with either:\n",
    "      - the requested number of communities, or\n",
    "      - the highest modularity seen within the first `max_splits` splits.\n",
    "    \"\"\"\n",
    "    from networkx.algorithms.community import girvan_newman, modularity\n",
    "\n",
    "    comp_gen = girvan_newman(G)\n",
    "    best_part, best_mod = None, -1.0\n",
    "\n",
    "    for i, communities in enumerate(islice(comp_gen, max_splits)):\n",
    "        partition = tuple(sorted(sorted(c) for c in communities))\n",
    "        if (\n",
    "            target_num_communities is not None\n",
    "            and len(partition) == target_num_communities\n",
    "        ):\n",
    "            return [set(c) for c in partition]\n",
    "        # track best modularity so far\n",
    "        m = modularity(G, communities, weight=\"weight\")\n",
    "        if m > best_mod:\n",
    "            best_mod, best_part = m, [set(c) for c in partition]\n",
    "\n",
    "    return best_part\n",
    "\n",
    "\n",
    "def _renormalize_rows(mat):\n",
    "    rs = mat.sum(axis=1, keepdims=True)\n",
    "    rs[rs == 0] = 1\n",
    "    return mat / rs\n",
    "\n",
    "\n",
    "def prune_transition_matrix(\n",
    "    transition_matrix,\n",
    "    prune_method,\n",
    "    # threshold method params\n",
    "    threshold=0.01,\n",
    "    # top_k method params\n",
    "    top_k=5,\n",
    "    # disparity filter params\n",
    "    alpha=0.05,\n",
    "    # centrality-based pruning params\n",
    "    centrality_threshold=0.1,\n",
    "    # top_genes method params\n",
    "    top_n_genes=200,\n",
    "    # --- NEW: community detection params ---\n",
    "    cd_method=\"girvan_newman\",\n",
    "    target_num_communities=None,  # e.g., 5; if None, picks best modularity\n",
    "    max_splits=50,  # how many GN splits to consider\n",
    "    use_abs_weights=True,  # use abs(weights) when building community graph\n",
    "    min_weight=0.0,  # ignore edges below this when building the graph\n",
    "    inter_strategy=\"remove\",  # \"remove\" | \"keep_ratio\"\n",
    "    inter_keep_ratio=0.0,  # if \"keep_ratio\", keep this fraction of strongest inter edges per row\n",
    "    intra_top_k=None,  # cap kept intra-community edges per row (None disables)\n",
    "    renormalize=True,  # re-normalize rows after pruning\n",
    "):\n",
    "    \"\"\"\n",
    "    Prunes edges from a transition matrix using different methods, including community detection.\n",
    "\n",
    "    Community detection (Girvanâ€“Newman) knobs:\n",
    "      - cd_method: currently only 'girvan_newman' supported\n",
    "      - target_num_communities: force a specific # of communities; if None, choose partition with best modularity\n",
    "      - max_splits: search depth for GN\n",
    "      - use_abs_weights: use |w| when forming the undirected graph for community detection\n",
    "      - min_weight: drop edges below this when forming the community graph\n",
    "      - inter_strategy: 'remove' to drop all inter-community edges, or 'keep_ratio' to keep a fraction per row\n",
    "      - inter_keep_ratio: fraction of strongest inter edges to retain per row if inter_strategy='keep_ratio'\n",
    "      - intra_top_k: keep at most K strongest intra-community edges per row (None = keep all)\n",
    "      - renormalize: L1 re-normalize rows post-pruning\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from scipy.stats import beta\n",
    "\n",
    "    def compute_centrality_measures(transition_matrix):\n",
    "        G = nx.from_numpy_array(transition_matrix, create_using=nx.DiGraph)\n",
    "        degree = nx.degree_centrality(G)\n",
    "        closeness = nx.closeness_centrality(\n",
    "            G, distance=None\n",
    "        )  # treat weights as strengths, not distances\n",
    "        betweenness = nx.betweenness_centrality(G, weight=\"weight\")\n",
    "        return degree, closeness, betweenness\n",
    "\n",
    "    if prune_method == \"no pruning\":\n",
    "        return transition_matrix\n",
    "\n",
    "    elif prune_method == \"top_genes\":\n",
    "        gene_strengths = np.sum(np.abs(transition_matrix), axis=1) + np.sum(\n",
    "            np.abs(transition_matrix), axis=0\n",
    "        )\n",
    "        top_gene_indices = np.argsort(gene_strengths)[-top_n_genes:]\n",
    "        pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        pruned_matrix[np.ix_(top_gene_indices, top_gene_indices)] = transition_matrix[\n",
    "            np.ix_(top_gene_indices, top_gene_indices)\n",
    "        ]\n",
    "\n",
    "    elif prune_method == \"threshold\":\n",
    "        pruned_matrix = np.where(\n",
    "            np.abs(transition_matrix) >= threshold, transition_matrix, 0\n",
    "        )\n",
    "\n",
    "    elif prune_method == \"top_k\":\n",
    "        pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        for i in range(transition_matrix.shape[0]):\n",
    "            row = transition_matrix[i, :]\n",
    "            k = min(top_k, row.size)\n",
    "            idx = np.argpartition(-np.abs(row), k - 1)[:k]\n",
    "            pruned_matrix[i, idx] = row[idx]\n",
    "\n",
    "    elif prune_method == \"disparity\":\n",
    "        n = transition_matrix.shape[0]\n",
    "        pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        abs_matrix = np.abs(transition_matrix)\n",
    "        strengths = np.sum(abs_matrix, axis=1, keepdims=True)\n",
    "        nonzero_counts = np.count_nonzero(abs_matrix, axis=1, keepdims=True)\n",
    "        mask_nonzero_strength = strengths > 0\n",
    "\n",
    "        for i in range(n):\n",
    "            if not mask_nonzero_strength[i, 0]:\n",
    "                continue\n",
    "            row = abs_matrix[i, :]\n",
    "            strength = strengths[i, 0]\n",
    "            k = int(nonzero_counts[i, 0])\n",
    "            if k == 0:\n",
    "                continue\n",
    "            nz = row > 0\n",
    "            p_ij = row[nz] / strength\n",
    "            alpha_ij = 1 - (k - 1) * beta.cdf(p_ij, 1, k - 1)\n",
    "            keep_mask = alpha_ij < alpha\n",
    "            keep_idx = np.where(nz)[0][keep_mask]\n",
    "            pruned_matrix[i, keep_idx] = transition_matrix[i, keep_idx]\n",
    "\n",
    "    elif prune_method == \"degree_centrality\":\n",
    "        degree, _, _ = compute_centrality_measures(transition_matrix)\n",
    "        pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        for i in range(transition_matrix.shape[0]):\n",
    "            if degree.get(i, 0) >= centrality_threshold:\n",
    "                pruned_matrix[i, :] = transition_matrix[i, :]\n",
    "\n",
    "    elif prune_method == \"closeness_centrality\":\n",
    "        _, closeness, _ = compute_centrality_measures(transition_matrix)\n",
    "        pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        for i in range(transition_matrix.shape[0]):\n",
    "            if closeness.get(i, 0) >= centrality_threshold:\n",
    "                pruned_matrix[i, :] = transition_matrix[i, :]\n",
    "\n",
    "    elif prune_method == \"betweenness_centrality\":\n",
    "        _, _, betweenness = compute_centrality_measures(transition_matrix)\n",
    "        pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        for i in range(transition_matrix.shape[0]):\n",
    "            if betweenness.get(i, 0) >= centrality_threshold:\n",
    "                pruned_matrix[i, :] = transition_matrix[i, :]\n",
    "\n",
    "    elif prune_method == \"community_detection\":\n",
    "        # 1) Build undirected weighted graph for community detection\n",
    "        W = np.abs(transition_matrix) if use_abs_weights else transition_matrix.copy()\n",
    "        if min_weight > 0:\n",
    "            W = np.where(W >= min_weight, W, 0.0)\n",
    "\n",
    "        # symmetrize for community detection\n",
    "        W_sym = np.maximum(W, W.T)\n",
    "        G = nx.from_numpy_array(W_sym)  # undirected, weight in 'weight'\n",
    "        # remove zero-weight edges to keep GN happy\n",
    "        G.remove_edges_from(\n",
    "            [(u, v) for u, v, d in G.edges(data=True) if d.get(\"weight\", 0.0) <= 0.0]\n",
    "        )\n",
    "\n",
    "        if G.number_of_edges() == 0:\n",
    "            pruned_matrix = np.zeros_like(transition_matrix)\n",
    "        else:\n",
    "            if cd_method != \"girvan_newman\":\n",
    "                raise ValueError(\"Only 'girvan_newman' is supported currently.\")\n",
    "            communities = _best_gn_partition(G, target_num_communities, max_splits)\n",
    "            # node -> community id\n",
    "            node2c = {}\n",
    "            for cid, cset in enumerate(communities):\n",
    "                for node in cset:\n",
    "                    node2c[node] = cid\n",
    "\n",
    "            # 2) Prune based on community structure\n",
    "            pruned_matrix = np.zeros_like(transition_matrix)\n",
    "\n",
    "            n = transition_matrix.shape[0]\n",
    "            for i in range(n):\n",
    "                row = transition_matrix[i, :]\n",
    "\n",
    "                # split indices by intra/inter relative to i's community\n",
    "                ci = node2c.get(i, -1)\n",
    "                intra_idx = [j for j in range(n) if node2c.get(j, -2) == ci]\n",
    "                inter_idx = [j for j in range(n) if node2c.get(j, -2) != ci]\n",
    "\n",
    "                # Intra: optionally cap top-K by strength\n",
    "                if intra_top_k is None or intra_top_k >= len(intra_idx):\n",
    "                    pruned_matrix[i, intra_idx] = row[intra_idx]\n",
    "                else:\n",
    "                    k = max(0, int(intra_top_k))\n",
    "                    if k > 0:\n",
    "                        intra_vals = np.abs(row[intra_idx])\n",
    "                        keep_local = np.argpartition(-intra_vals, k - 1)[:k]\n",
    "                        keep_idx = [intra_idx[t] for t in keep_local]\n",
    "                        pruned_matrix[i, keep_idx] = row[keep_idx]\n",
    "\n",
    "                # Inter: drop or keep a fraction of strongest\n",
    "                if inter_strategy == \"remove\":\n",
    "                    pass  # keep zeros\n",
    "                elif inter_strategy == \"keep_ratio\":\n",
    "                    frac = float(inter_keep_ratio)\n",
    "                    if 0.0 < frac <= 1.0 and len(inter_idx) > 0:\n",
    "                        k = max(1, int(np.ceil(frac * len(inter_idx))))\n",
    "                        inter_vals = np.abs(row[inter_idx])\n",
    "                        keep_local = np.argpartition(\n",
    "                            -inter_vals, min(k - 1, len(inter_vals) - 1)\n",
    "                        )[:k]\n",
    "                        keep_idx = [inter_idx[t] for t in keep_local]\n",
    "                        pruned_matrix[i, keep_idx] = row[keep_idx]\n",
    "                else:\n",
    "                    raise ValueError(\"inter_strategy must be 'remove' or 'keep_ratio'.\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pruning method: {prune_method}\")\n",
    "\n",
    "    # Optional row re-normalization to keep a valid transition matrix\n",
    "    if renormalize:\n",
    "        pruned_matrix = _renormalize_rows(pruned_matrix)\n",
    "\n",
    "    return pruned_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency_matrix(adjacency_matrix):\n",
    "    row_sums = adjacency_matrix.sum(axis=1, keepdims=True)\n",
    "    # Avoid division by zero for genes with no outgoing edges\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    transition_matrix = adjacency_matrix / row_sums\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def simulate_tf_perturbation(\n",
    "    adjacency_matrix,\n",
    "    tf_gene_index,\n",
    "    perturbation_strength=1.0,\n",
    "    damping_factor=0.85,\n",
    "    convergence_tolerance=1e-6,\n",
    "    max_iterations=100,\n",
    "    prune_method=\"threshold\",\n",
    "    prune_params={},\n",
    "):\n",
    "    num_genes = adjacency_matrix.shape[0]\n",
    "\n",
    "    # Initialize perturbation vector (only the TF is initially perturbed)\n",
    "    initial_perturbation = np.zeros(num_genes)\n",
    "    initial_perturbation[tf_gene_index] = perturbation_strength\n",
    "\n",
    "    # Create transition matrix (transpose for correct direction of influence)\n",
    "    transition_matrix = normalize_adjacency_matrix(adjacency_matrix).T\n",
    "\n",
    "    transition_matrix = prune_transition_matrix(\n",
    "        transition_matrix, prune_method, **prune_params\n",
    "    )\n",
    "\n",
    "    # Initialize current perturbation state\n",
    "    current_perturbation = initial_perturbation.copy()\n",
    "\n",
    "    # Iterative damped propagation\n",
    "    for iteration in tqdm(\n",
    "        range(max_iterations), desc=\"Simulating perturbation propagation\"\n",
    "    ):\n",
    "        # Propagate through network\n",
    "        propagated = transition_matrix @ current_perturbation\n",
    "\n",
    "        # Combine propagated signal with source retention\n",
    "        next_perturbation = damping_factor * propagated\n",
    "        next_perturbation[tf_gene_index] += (1 - damping_factor) * perturbation_strength\n",
    "\n",
    "        # Check for convergence\n",
    "        perturbation_change = np.linalg.norm(\n",
    "            next_perturbation - current_perturbation, ord=1\n",
    "        )\n",
    "        if perturbation_change < convergence_tolerance:\n",
    "            break\n",
    "\n",
    "        current_perturbation = next_perturbation\n",
    "\n",
    "    return current_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pruning methods to compare - using top_genes method to get exactly ~200 genes\n",
    "print(f\"Network shape: {gene_regulatory_network.shape}\")\n",
    "print(f\"Total edges: {np.count_nonzero(gene_regulatory_network):,}\")\n",
    "print(f\"Max edge weight: {gene_regulatory_network.max():.6f}\")\n",
    "\n",
    "# Calculate gene strengths to see distribution\n",
    "gene_strengths = np.sum(np.abs(gene_regulatory_network), axis=1) + np.sum(\n",
    "    np.abs(gene_regulatory_network), axis=0\n",
    ")\n",
    "print(f\"Gene strength - 95th percentile: {np.percentile(gene_strengths, 95):.4f}\")\n",
    "print(f\"Gene strength - 99th percentile: {np.percentile(gene_strengths, 99):.4f}\")\n",
    "print(f\"Top 200 gene strength threshold: {np.sort(gene_strengths)[-200]:.4f}\")\n",
    "\n",
    "pruning_methods = {\n",
    "    \"no_pruning\": {\"method\": \"no pruning\", \"params\": {}},\n",
    "    \"girvan newmann\": {\n",
    "        \"method\": \"community_detection\",\n",
    "        \"params\": {\"inter_strategy\": \"remove\", \"intra_top_k\": 50},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"\\nComparing {len(pruning_methods)} pruning methods\")\n",
    "print(f\"Target TF: {target_tf_name}\")\n",
    "print(\"Targeting ~200-300 genes with connections after pruning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94679188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perturbation scores for each method\n",
    "perturbation_results = {}\n",
    "\n",
    "\n",
    "# Helper function to count genes with connections\n",
    "def count_active_genes(matrix):\n",
    "    \"\"\"Count genes that have at least one outgoing or incoming connection\"\"\"\n",
    "    outgoing = np.count_nonzero(np.sum(matrix, axis=1))\n",
    "    incoming = np.count_nonzero(np.sum(matrix, axis=0))\n",
    "    return max(outgoing, incoming)\n",
    "\n",
    "\n",
    "for method_name, config in pruning_methods.items():\n",
    "    print(f\"\\nComputing perturbations for: {method_name}\")\n",
    "\n",
    "    # Test the pruning first to see network size\n",
    "    transition_matrix = normalize_adjacency_matrix(gene_regulatory_network).T\n",
    "    pruned_matrix = prune_transition_matrix(\n",
    "        transition_matrix, config[\"method\"], **config[\"params\"]\n",
    "    )\n",
    "\n",
    "    active_genes = count_active_genes(pruned_matrix)\n",
    "    edges_remaining = np.count_nonzero(pruned_matrix)\n",
    "\n",
    "    print(f\"  Active genes after pruning: {active_genes}\")\n",
    "    print(f\"  Edges remaining: {edges_remaining:,}\")\n",
    "    print(f\"  Network density: {edges_remaining / pruned_matrix.size * 100:.4f}%\")\n",
    "\n",
    "    # Run perturbation simulation\n",
    "    scores = simulate_tf_perturbation(\n",
    "        gene_regulatory_network,\n",
    "        target_tf_index,\n",
    "        perturbation_strength=1.0,\n",
    "        prune_method=config[\"method\"],\n",
    "        prune_params=config[\"params\"],\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    perturbation_results[method_name] = {\n",
    "        \"scores\": scores,\n",
    "        \"target_tf_score\": scores[target_tf_index],\n",
    "        \"max_score\": np.max(np.abs(scores)),\n",
    "        \"mean_score\": np.mean(np.abs(scores)),\n",
    "        \"active_genes\": active_genes,\n",
    "        \"edges_remaining\": edges_remaining,\n",
    "    }\n",
    "\n",
    "print(f\"\\nComputed perturbations for {len(perturbation_results)} methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualization adapted for different parameter combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Create network plots for each method\n",
    "methods = list(perturbation_results.keys())\n",
    "fig, axes = plt.subplots(1, len(methods), figsize=(18, 6), dpi=100)\n",
    "\n",
    "# If only one method, make axes a list\n",
    "if len(methods) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, method_name in zip(axes, methods):\n",
    "    # Get perturbation scores for this method\n",
    "    scores = perturbation_results[method_name][\"scores\"]\n",
    "\n",
    "    # Create network graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes with perturbation scores\n",
    "    for i, gene in enumerate(genes):\n",
    "        G.add_node(gene, p=scores[i])\n",
    "\n",
    "    # Add edges using 95th percentile threshold from original network\n",
    "    thresh = np.percentile(gene_regulatory_network, 95)\n",
    "    for i in range(gene_regulatory_network.shape[0]):\n",
    "        for j in range(gene_regulatory_network.shape[1]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if gene_regulatory_network[i, j] >= thresh:\n",
    "                G.add_edge(genes[i], genes[j])\n",
    "\n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "    # Get perturbation values for coloring\n",
    "    vals = [G.nodes[n][\"p\"] for n in G.nodes()]\n",
    "\n",
    "    # Draw the network\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos,\n",
    "        ax=ax,\n",
    "        node_color=vals,\n",
    "        cmap=\"coolwarm\",\n",
    "        with_labels=False,\n",
    "        node_size=100,\n",
    "        edge_color=\"lightgray\",\n",
    "    )\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(f\"{method_name} perturbation ({target_tf_name})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare perturbation distributions and top affected genes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Target TF response comparison\n",
    "ax1 = axes[0, 0]\n",
    "methods = list(perturbation_results.keys())\n",
    "tf_scores = [perturbation_results[m][\"target_tf_score\"] for m in methods]\n",
    "\n",
    "bars = ax1.bar(methods, tf_scores, alpha=0.7, color=\"skyblue\")\n",
    "ax1.set_ylabel(f\"{target_tf_name} Perturbation Score\")\n",
    "ax1.set_title(f\"Target TF ({target_tf_name}) Response by Method\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, tf_scores):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.001,\n",
    "        f\"{score:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# 2. Perturbation score distributions\n",
    "ax2 = axes[0, 1]\n",
    "score_data = [perturbation_results[m][\"scores\"] for m in methods]\n",
    "box_plot = ax2.boxplot(score_data, labels=methods, patch_artist=True)\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))\n",
    "for patch, color in zip(box_plot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel(\"Perturbation Score Distribution\")\n",
    "ax2.set_title(\"Score Distribution Comparison\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Top 20 affected genes heatmap - use the bottom row\n",
    "# Create a new subplot spanning the bottom row\n",
    "ax3 = plt.subplot(2, 1, 2)  # This creates a subplot in the bottom half\n",
    "all_scores = np.column_stack([perturbation_results[m][\"scores\"] for m in methods])\n",
    "\n",
    "# Find top 20 most variable genes across methods\n",
    "gene_variance = np.var(all_scores, axis=1)\n",
    "top_gene_indices = np.argsort(gene_variance)[::-1][:20]\n",
    "top_genes = [genes[i] for i in top_gene_indices]\n",
    "heatmap_data = all_scores[top_gene_indices, :]\n",
    "\n",
    "im = ax3.imshow(\n",
    "    heatmap_data,\n",
    "    cmap=\"RdBu_r\",\n",
    "    aspect=\"auto\",\n",
    "    vmin=-np.max(np.abs(heatmap_data)),\n",
    "    vmax=np.max(np.abs(heatmap_data)),\n",
    ")\n",
    "\n",
    "ax3.set_xticks(range(len(methods)))\n",
    "ax3.set_xticklabels(methods, rotation=45, ha=\"right\")\n",
    "ax3.set_yticks(range(len(top_genes)))\n",
    "ax3.set_yticklabels(top_genes, fontsize=8)\n",
    "ax3.set_title(\"Top 20 Most Variable Genes Across Methods\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3, shrink=0.8)\n",
    "cbar.set_label(\"Perturbation Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method similarity and consensus analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Method correlation matrix\n",
    "ax1 = axes[0]\n",
    "methods = list(perturbation_results.keys())\n",
    "all_scores = np.column_stack([perturbation_results[m][\"scores\"] for m in methods])\n",
    "correlation_matrix = np.corrcoef(all_scores.T)\n",
    "\n",
    "im1 = ax1.imshow(correlation_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "ax1.set_xticks(range(len(methods)))\n",
    "ax1.set_xticklabels(methods, rotation=45, ha=\"right\", fontsize=8)\n",
    "ax1.set_yticks(range(len(methods)))\n",
    "ax1.set_yticklabels(methods, fontsize=8)\n",
    "ax1.set_title(\"Method Similarity\\n(Correlation)\")\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(methods)):\n",
    "    for j in range(len(methods)):\n",
    "        ax1.text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{correlation_matrix[i, j]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\" if abs(correlation_matrix[i, j]) > 0.5 else \"black\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "\n",
    "# 2. Find consensus genes (consistently high across methods)\n",
    "ax2 = axes[1]\n",
    "# Normalize scores and find genes with high impact across multiple methods\n",
    "normalized_scores = np.abs(all_scores) / np.max(np.abs(all_scores), axis=0)\n",
    "consensus_scores = np.mean(\n",
    "    normalized_scores > 0.5, axis=1\n",
    ")  # Fraction of methods with high impact\n",
    "\n",
    "# Get top 15 consensus genes\n",
    "top_consensus_indices = np.argsort(consensus_scores)[::-1][:15]\n",
    "top_consensus_genes = [genes[i] for i in top_consensus_indices]\n",
    "top_consensus_values = consensus_scores[top_consensus_indices]\n",
    "\n",
    "y_pos = np.arange(len(top_consensus_genes))\n",
    "bars = ax2.barh(y_pos, top_consensus_values, alpha=0.7, color=\"lightgreen\")\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(top_consensus_genes)\n",
    "ax2.set_xlabel(\"Consensus Score (Fraction of Methods)\")\n",
    "ax2.set_title(\"Top Consensus Genes\\n(Consistently Affected)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Network complexity vs performance\n",
    "ax3 = axes[2]\n",
    "# Calculate network statistics for each method\n",
    "network_stats = {}\n",
    "for method_name, config in pruning_methods.items():\n",
    "    transition_matrix = normalize_adjacency_matrix(gene_regulatory_network).T\n",
    "    pruned_matrix = prune_transition_matrix(\n",
    "        transition_matrix, config[\"method\"], **config[\"params\"]\n",
    "    )\n",
    "\n",
    "    n_edges = np.count_nonzero(pruned_matrix)\n",
    "    max_score = perturbation_results[method_name][\"max_score\"]\n",
    "\n",
    "    network_stats[method_name] = {\"edges\": n_edges, \"max_score\": max_score}\n",
    "\n",
    "edge_counts = [network_stats[m][\"edges\"] for m in methods]\n",
    "max_scores = [network_stats[m][\"max_score\"] for m in methods]\n",
    "\n",
    "scatter = ax3.scatter(\n",
    "    edge_counts, max_scores, s=100, alpha=0.7, c=range(len(methods)), cmap=\"viridis\"\n",
    ")\n",
    "for i, method in enumerate(methods):\n",
    "    ax3.annotate(\n",
    "        method,\n",
    "        (edge_counts[i], max_scores[i]),\n",
    "        xytext=(5, 5),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "ax3.set_xlabel(\"Number of Edges (After Pruning)\")\n",
    "ax3.set_ylabel(\"Maximum Perturbation Score\")\n",
    "ax3.set_title(\"Network Complexity vs\\nPerturbation Strength\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print simple summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERTURBATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTarget TF: {target_tf_name}\")\n",
    "print(\n",
    "    f\"Best method by TF response: {max(methods, key=lambda m: abs(perturbation_results[m]['target_tf_score']))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Best method by max perturbation: {max(methods, key=lambda m: perturbation_results[m]['max_score'])}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 5 consensus genes:\")\n",
    "for i, gene in enumerate(top_consensus_genes[:5], 1):\n",
    "    print(f\"  {i}. {gene}\")\n",
    "\n",
    "print(f\"\\nMethod correlations (most similar pair):\")\n",
    "max_corr = -1\n",
    "best_pair = None\n",
    "for i in range(len(methods)):\n",
    "    for j in range(i + 1, len(methods)):\n",
    "        if correlation_matrix[i, j] > max_corr:\n",
    "            max_corr = correlation_matrix[i, j]\n",
    "            best_pair = (methods[i], methods[j])\n",
    "\n",
    "if best_pair:\n",
    "    print(f\"  {best_pair[0]} â†” {best_pair[1]}: {max_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in simple format\n",
    "output_dir = \"../../results/grn_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for method in methods:\n",
    "    summary_data.append({\n",
    "        'Method': method,\n",
    "        'Target_TF_Score': perturbation_results[method]['target_tf_score'],\n",
    "        'Max_Perturbation': perturbation_results[method]['max_score'],\n",
    "        'Mean_Perturbation': perturbation_results[method]['mean_score'],\n",
    "        'Network_Edges': network_stats[method]['edges']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Target_TF_Score', key=abs, ascending=False)\n",
    "\n",
    "print(\"Method Comparison Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(summary_df.round(4).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_file = os.path.join(output_dir, f\"perturbation_summary_{target_tf_name}.csv\")\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "# Save perturbation matrix\n",
    "perturbation_matrix = np.column_stack([perturbation_results[m]['scores'] for m in methods])\n",
    "matrix_file = os.path.join(output_dir, f\"perturbation_matrix_{target_tf_name}.npz\")\n",
    "np.savez_compressed(matrix_file, \n",
    "                   matrix=perturbation_matrix,\n",
    "                   methods=methods,\n",
    "                   genes=genes,\n",
    "                   target_tf=target_tf_name,\n",
    "                   target_tf_index=target_tf_index)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved:\")\n",
    "print(f\"  Summary: {summary_file}\")\n",
    "print(f\"  Matrix: {matrix_file}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Analysis complete! Target TF: {target_tf_name}\")\n",
    "print(f\"\udcca {len(methods)} methods compared across {len(genes)} genes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimenting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
